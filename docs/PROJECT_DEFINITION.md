# ğŸ“‹ ML Project Definition Document
## Credit Card Fraud Detection System

**Document Version:** 1.0
**Date:** February 2025
**Author:** ML Strategy Team
**Status:** âœ… Approved for Development

---

## 1. EXECUTIVE SUMMARY

### Problem Statement
Our organization is experiencing significant financial losses due to credit card fraud, with an estimated annual impact of $2M in fraudulent transactions. The current rule-based detection system flags 5% of all transactions for manual review, but 60% of these flagged transactions turn out to be legitimate (false positives), creating operational inefficiency and customer friction. Additionally, an unknown but substantial portion of actual fraud passes through undetected.

### Proposed Solution
Implement a machine learning-based fraud detection system using ensemble methods (Logistic Regression, Random Forest, XGBoost) trained on historical transaction data. The system will leverage multiple techniques to handle the extreme class imbalance (99.83% normal vs 0.17% fraud): SMOTE oversampling, random undersampling, and cost-sensitive learning with class weights.

### Expected Impact

| Impact Area | Current State | Target State | Annual Value |
|-------------|---------------|--------------|--------------|
| **Primary:** Fraud Loss Reduction | $2M loss | $1.2M loss (40% â†“) | **$800K saved** |
| **Secondary:** Manual Review Reduction | 50,000 reviews (60% FP) | 25,000 reviews (30% FP) | **$250K saved** |
| **Tertiary:** Customer Experience | 3% false decline rate | 2% false decline rate | **Brand value** |
| **Total Quantifiable Savings** | - | - | **$1.05M/year** |

### ROI Analysis
- **Investment:** $200K (6 months development + infrastructure)
- **Annual Savings:** $1.05M
- **ROI:** 5.25:1 (first year), 10.5:1 (second year onwards)
- **Payback Period:** 2.3 months

### Timeline
- **MVP (Minimum Viable Model):** 4 weeks
- **Production-Ready:** 8 weeks
- **Full Deployment:** 10 weeks

---

## 2. BUSINESS OBJECTIVES (Prioritized)

### ğŸ¯ Level 6 - CRITICAL (Must Achieve)

**Objective 2.1: Reduce Direct Fraud Losses**
| Attribute | Value |
|-----------|-------|
| **Current State** | $2M annual fraud loss |
| **Target State** | $1.2M annual loss (40% reduction) |
| **Minimum Acceptable** | $1.4M annual loss (30% reduction) |
| **Metric** | Total fraud dollars detected before authorization |
| **Measurement** | Monthly fraud chargebacks + detected fraud value |
| **Impact** | Direct P&L impact, board-level visibility |
| **Owner** | CFO |

**Objective 2.2: Maintain Transaction Approval Speed**
| Attribute | Value |
|-----------|-------|
| **Current State** | < 50ms average decision time |
| **Target State** | < 100ms average decision time |
| **Minimum Acceptable** | < 150ms (p99) |
| **Metric** | Transaction authorization latency |
| **Measurement** | Real-time monitoring, APM tools |
| **Impact** | Customer experience, checkout abandonment |
| **Owner** | VP Engineering |

---

### ğŸ¯ Level 5 - HIGH PRIORITY

**Objective 2.3: Reduce False Positive Rate**
| Attribute | Value |
|-----------|-------|
| **Current State** | 60% false positive rate (30,000 wasted reviews/year) |
| **Target State** | 30% false positive rate (15,000 reviews saved) |
| **Minimum Acceptable** | 40% false positive rate |
| **Metric** | Precision of fraud predictions |
| **Measurement** | Weekly precision calculation on flagged transactions |
| **Impact** | Fraud team productivity, $250K operational cost |
| **Owner** | Fraud Operations Manager |

**Objective 2.4: Improve Fraud Catch Rate**
| Attribute | Value |
|-----------|-------|
| **Current State** | Unknown (estimated 60-70% catch rate) |
| **Target State** | 85% catch rate |
| **Minimum Acceptable** | 75% catch rate |
| **Metric** | Recall of fraud predictions |
| **Measurement** | Monthly analysis of chargebacks vs predictions |
| **Impact** | Direct fraud loss reduction |
| **Owner** | Chief Risk Officer |

---

### ğŸ¯ Level 4 - IMPORTANT

**Objective 2.5: Reduce Customer Friction**
| Attribute | Value |
|-----------|-------|
| **Current State** | 3% false decline rate (30,000 customers/year) |
| **Target State** | 2% false decline rate |
| **Minimum Acceptable** | 2.5% false decline rate |
| **Metric** | Legitimate transactions incorrectly blocked |
| **Measurement** | Customer complaints + manual override analysis |
| **Impact** | Customer satisfaction, NPS, churn prevention |
| **Owner** | VP Customer Experience |

---

### ğŸ¯ Level 3 - MODERATE

**Objective 2.6: Enable Regulatory Compliance**
| Attribute | Value |
|-----------|-------|
| **Current State** | Manual audit trail, limited explainability |
| **Target State** | Automated audit logs, explainable decisions |
| **Minimum Acceptable** | Basic decision rationale available |
| **Metric** | Audit compliance score |
| **Measurement** | Quarterly compliance review |
| **Impact** | Regulatory risk, potential fines |
| **Owner** | Chief Compliance Officer |

---

## 3. TECHNICAL METRICS MAPPING

### Primary Metrics Table

| Business KPI | ML Metric | Baseline | MVP | Target | Stretch | Measurement Method |
|--------------|-----------|----------|-----|--------|---------|-------------------|
| Fraud $ caught | **Recall** | ~65% | 75% | 85% | 90% | TP / (TP + FN) on test set |
| Review efficiency | **Precision** | 15% | 50% | 60% | 70% | TP / (TP + FP) on test set |
| Overall quality | **F1-Score** | ~25% | 65% | 70% | 80% | 2 Ã— (P Ã— R) / (P + R) |
| Ranking quality | **PR-AUC** | ~0.55 | 0.75 | 0.80 | 0.85 | Area under PR curve |
| Real-time capability | **Latency** | N/A | <100ms | <50ms | <20ms | p99 inference time |

### Why These Metrics (NOT Accuracy)

```
âš ï¸ WARNING: DO NOT USE ACCURACY FOR THIS PROBLEM

With 99.83% normal transactions:
- A model that predicts EVERYTHING as "normal" achieves 99.83% accuracy
- But catches 0% of fraud = completely useless
- This is the #1 mistake in imbalanced classification

âœ… USE INSTEAD:
- Recall: "Of all actual frauds, how many did we catch?"
- Precision: "Of all transactions we flagged, how many were actual fraud?"
- F1-Score: Balances both (harmonic mean prevents gaming)
- PR-AUC: Overall ranking quality, robust to imbalance
```

### Metric Priority for Fraud Detection

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     FRAUD DETECTION PRIORITY        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    RECALL (Catch Rate)          PRECISION (Efficiency)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Missing fraud = $$$          False alarm = wasted time

    For financial institutions:
    Cost of missed fraud ($500 avg) >> Cost of review ($10)

    Therefore: Recall > Precision (but both matter)

    Optimal threshold: Maximize F1-Score
    Business adjustment: May lower threshold to increase recall
```

### Confusion Matrix Business Translation

```
                        PREDICTED
                    Normal    Fraud
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Normal    â”‚   TN    â”‚   FP    â”‚  â† False Positive: Annoyed customer,
ACTUAL        â”‚ (good)  â”‚ (waste) â”‚    wasted review time ($10/review)
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    Fraud     â”‚   FN    â”‚   TP    â”‚  â† False Negative: FRAUD PASSES THROUGH
              â”‚ (BAD!)  â”‚ (good)  â”‚    Direct $ loss ($500 avg)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Business Impact Matrix:
    - TN: No cost (normal transaction approved correctly)
    - TP: Fraud caught (saves $500 avg)
    - FP: Wasted review (costs $10) + Customer friction
    - FN: Missed fraud (costs $500 avg) â† WORST OUTCOME

    Cost Ratio: FN is 50x worse than FP
```

---

## 4. CONSTRAINTS & ASSUMPTIONS

### Technical Constraints

#### âœ… Verified Constraints

| Constraint | Value | Verification | Implication |
|------------|-------|--------------|-------------|
| Dataset Size | 284,807 transactions | Confirmed | Sufficient for tree-based models, marginal for deep learning |
| Fraud Rate | 0.17% (492 cases) | Confirmed | Extreme imbalance, requires special handling |
| Features | 30 (V1-V28 PCA + Time + Amount) | Confirmed | No raw features available, limited feature engineering |
| Latency Requirement | < 100ms | Business requirement | Rules out complex ensemble stacking, favors single optimized model |
| Memory Limit | 8GB RAM | Infrastructure | XGBoost/RF feasible, limits batch sizes |

#### âš ï¸ Assumptions to Validate

| Assumption | Risk if Wrong | Validation Method | Validation Timeline |
|------------|---------------|-------------------|---------------------|
| PCA features (V1-V28) contain fraud signal | Model won't learn patterns | Feature importance analysis | Week 2 |
| Historical patterns predict future fraud | Model drift, poor production performance | Temporal validation split | Week 3 |
| Fraud types are homogeneous | Single model inadequate | Cluster analysis of fraud cases | Week 2 |
| No seasonal patterns | Degraded performance in some periods | Time-series analysis of fraud rate | Week 2 |
| Data is representative of production | Deployment failures | Compare with recent production sample | Week 4 |

### Business Constraints

| Constraint | Impact | Mitigation |
|------------|--------|------------|
| Budget: $200K total | Limits team size, cloud spend | Prioritize high-impact features, use spot instances |
| Timeline: 8 weeks to production | No time for exotic approaches | Focus on proven methods (RF, XGBoost), skip deep learning |
| Maintenance: 0.5 FTE allocated | Limited monitoring capacity | Automate retraining pipeline, simple alerting |
| Stakeholder availability: 5 hrs/week | Delayed feedback cycles | Weekly sync meetings, async documentation |

### Regulatory Constraints

| Regulation | Requirement | Implementation |
|------------|-------------|----------------|
| **PCI-DSS** | Secure handling of cardholder data | Data encrypted at rest and in transit, access logging |
| **GDPR (if EU customers)** | Right to explanation for automated decisions | SHAP values for decision explanation, human override available |
| **Fair Lending Laws** | No discrimination on protected attributes | Bias testing across demographics (if data available) |
| **SOX Compliance** | Audit trail for financial decisions | Logging all predictions with timestamps, model versions |

---

## 5. RISK ASSESSMENT & MITIGATION

### Risk Heat Map

```
                    IMPACT
           Low      Medium     High
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    High â”‚   5    â”‚   4    â”‚  1,2   â”‚  â† Address immediately
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
PROB Med â”‚   6    â”‚   3    â”‚        â”‚  â† Monitor closely
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    Low  â”‚        â”‚        â”‚        â”‚  â† Accept
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
1 = Data Quality Issues
2 = Class Imbalance Mishandled
3 = Data Leakage
4 = Model Drift
5 = Latency Issues
6 = Interpretability Concerns
```

---

### ğŸ”´ HIGH RISKS (Immediate Attention Required)

#### RISK 1: Data Quality Issues

| Attribute | Details |
|-----------|---------|
| **Description** | Missing values, outliers, duplicates, or inconsistent data formats that compromise model training |
| **Probability** | 70% (most ML projects encounter this) |
| **Impact** | ğŸ”¥ğŸ”¥ğŸ”¥ PROJECT FAILURE - Garbage in = garbage out |
| **Root Cause** | Data pipeline issues, collection errors, ETL bugs |

**Mitigation Plan:**
```
Week 1-2: Comprehensive Data Quality Audit
â”œâ”€â”€ Check 1: Missing values per column (threshold: < 5%)
â”œâ”€â”€ Check 2: Duplicate transactions (threshold: 0)
â”œâ”€â”€ Check 3: Outlier analysis (Amount, Time distributions)
â”œâ”€â”€ Check 4: Class label consistency
â”œâ”€â”€ Check 5: Feature value ranges (V1-V28 should be standardized)
â””â”€â”€ Check 6: Temporal consistency (no future data leakage)

Deliverable: Data Quality Report with pass/fail status
```

**Go/No-Go Criteria:**
- âœ… PROCEED: < 5% missing, 0 duplicates, distributions as expected
- ğŸ›‘ STOP: > 20% data unusable â†’ Fix data pipeline first (add 2-4 weeks)
- â†» PIVOT: 5-20% issues â†’ Imputation strategy + reduced confidence in results

**Owner:** Data Engineer
**Status:** ğŸŸ¡ Pending (Week 1-2)

---

#### RISK 2: Class Imbalance Mishandled

| Attribute | Details |
|-----------|---------|
| **Description** | With 99.83% normal transactions, naive models predict everything as "normal" and achieve 99.83% "accuracy" while catching 0% fraud |
| **Probability** | 90% if not explicitly addressed |
| **Impact** | ğŸ”¥ğŸ”¥ğŸ”¥ USELESS MODEL - High accuracy, zero business value |
| **Root Cause** | Default ML algorithms optimize for accuracy |

**Mitigation Plan:**
```
Strategy 1: SMOTE (Synthetic Minority Over-sampling)
â”œâ”€â”€ Generate synthetic fraud samples
â”œâ”€â”€ Sampling ratio: 0.5 (fraud becomes 33% of training data)
â””â”€â”€ Validation: Compare with original class distribution

Strategy 2: Random Undersampling
â”œâ”€â”€ Reduce normal transactions to match fraud count
â”œâ”€â”€ Trade-off: Lose information from normal transactions
â””â”€â”€ Use when: Training speed is critical

Strategy 3: Class Weights (Recommended for production)
â”œâ”€â”€ class_weight='balanced' in sklearn
â”œâ”€â”€ scale_pos_weight in XGBoost (auto-calculated)
â””â”€â”€ Advantage: No data manipulation, works on full dataset

Evaluation Protocol:
â”œâ”€â”€ NEVER use accuracy as primary metric
â”œâ”€â”€ Primary: F1-Score (â‰¥ 65% MVP, â‰¥ 70% target)
â”œâ”€â”€ Secondary: PR-AUC (â‰¥ 0.75 MVP, â‰¥ 0.80 target)
â””â”€â”€ Sanity check: Recall > 0 (model actually detects fraud)
```

**Go/No-Go Criteria:**
- âœ… PROCEED: F1-Score â‰¥ 65% with any sampling strategy
- ğŸ›‘ STOP: F1-Score < 50% after trying all strategies â†’ Problem may not be solvable with this data
- â†» PIVOT: One strategy works, others don't â†’ Commit to working strategy

**Owner:** ML Engineer
**Status:** ğŸŸ¢ Strategy Defined (implemented in codebase)

---

#### RISK 3: Data Leakage

| Attribute | Details |
|-----------|---------|
| **Description** | Information from the future or test set leaks into training, causing inflated validation metrics that don't hold in production |
| **Probability** | 40% (common mistake, especially with time-series data) |
| **Impact** | ğŸ”¥ğŸ”¥ PRODUCTION FAILURE - Model performs 10-20% worse than expected |
| **Root Cause** | Improper train/test split, feature engineering on full dataset |

**Common Leakage Sources in Fraud Detection:**
```
âŒ WRONG: Random train/test split
   - Future transactions in training predict past transactions in test
   - Leaks temporal patterns

âŒ WRONG: Fit scaler on full dataset, then split
   - Test set statistics influence training
   - Artificially good normalization

âŒ WRONG: Use aggregated features (e.g., "user's average transaction")
   - Aggregates include future transactions
   - Model "knows" future behavior

âœ… CORRECT: Temporal split (train on old, test on new)
âœ… CORRECT: Fit preprocessors ONLY on training data
âœ… CORRECT: Feature engineering AFTER split, per-set
```

**Mitigation Plan:**
```
Protocol 1: Strict Temporal Validation
â”œâ”€â”€ Sort transactions by time
â”œâ”€â”€ Train: First 80% of transactions (older)
â”œâ”€â”€ Test: Last 20% of transactions (newer)
â””â”€â”€ Never shuffle before split

Protocol 2: Pipeline Discipline
â”œâ”€â”€ All preprocessing in sklearn Pipeline
â”œâ”€â”€ fit_transform ONLY on train
â”œâ”€â”€ transform ONLY on test
â””â”€â”€ Code review checklist for leakage

Protocol 3: Sanity Checks
â”œâ”€â”€ If validation >> training performance â†’ Suspect leakage
â”œâ”€â”€ If simple model matches complex model â†’ Suspect leakage
â”œâ”€â”€ Compare production metrics to validation within 2 weeks
â””â”€â”€ If production < 80% of validation â†’ Investigate immediately
```

**Go/No-Go Criteria:**
- âœ… PROCEED: Validation and temporal test performance within 5%
- ğŸ›‘ STOP: Validation performance implausibly high (>95% F1) â†’ Definitely leakage
- â†» PIVOT: Production performance significantly lower â†’ Audit feature engineering

**Owner:** ML Engineer
**Status:** ğŸŸ¢ Protocol Established

---

### ğŸŸ¡ MEDIUM RISKS (Monitor Closely)

#### RISK 4: Model Drift

| Attribute | Details |
|-----------|---------|
| **Description** | Fraud patterns evolve over time; fraudsters adapt to detection methods |
| **Probability** | 100% (WILL happen, only question is when) |
| **Impact** | ğŸ”¥ GRADUAL DEGRADATION - Performance decays 5-10% per quarter if not addressed |
| **Timeline** | Typically noticeable within 3-6 months |

**Monitoring Plan:**
```
Daily Metrics (Automated Alerts):
â”œâ”€â”€ Prediction volume (sudden changes = distribution shift)
â”œâ”€â”€ Average fraud probability (drift indicator)
â”œâ”€â”€ Latency percentiles (performance degradation)
â””â”€â”€ Alert threshold: > 2 standard deviations from baseline

Weekly Metrics (Dashboard Review):
â”œâ”€â”€ Precision, Recall, F1-Score (requires labeled data)
â”œâ”€â”€ Confusion matrix changes
â”œâ”€â”€ Feature distribution comparisons
â””â”€â”€ Review meeting: Every Monday, 30 minutes

Monthly Actions:
â”œâ”€â”€ Full model performance audit
â”œâ”€â”€ Compare to baseline established at deployment
â”œâ”€â”€ Trigger retraining if F1 drops > 5%
â””â”€â”€ Document: Model Performance Log
```

**Retraining Strategy:**
```
Automatic Retraining Triggers:
â”œâ”€â”€ F1-Score drops > 5% from baseline
â”œâ”€â”€ Precision OR Recall drops > 10%
â”œâ”€â”€ New fraud pattern identified by fraud team
â””â”€â”€ Quarterly scheduled retrain (regardless of metrics)

Retraining Process:
â”œâ”€â”€ Collect last 6 months of labeled data
â”œâ”€â”€ Retrain with same hyperparameters
â”œâ”€â”€ A/B test new model vs current (10% traffic)
â”œâ”€â”€ If new model better â†’ Gradual rollout (25% â†’ 50% â†’ 100%)
â””â”€â”€ Keep previous model as fallback
```

**Owner:** ML Engineer + Fraud Team
**Status:** ğŸŸ¡ Plan Defined (implement post-deployment)

---

#### RISK 5: Latency Issues

| Attribute | Details |
|-----------|---------|
| **Description** | Model inference too slow for real-time transaction approval |
| **Probability** | 30% |
| **Impact** | ğŸ”¥ CANNOT DEPLOY - Transaction approval requires < 100ms |

**Mitigation Plan:**
```
Phase 1: Benchmark Early (Week 3)
â”œâ”€â”€ Measure inference time on sample data
â”œâ”€â”€ Test on production-equivalent hardware
â”œâ”€â”€ Target: p99 < 100ms for single prediction
â””â”€â”€ Document: Latency Benchmark Report

Phase 2: Optimization (if needed)
â”œâ”€â”€ Model compression (reduce trees, depth)
â”œâ”€â”€ Feature selection (remove low-importance features)
â”œâ”€â”€ Quantization (float32 â†’ float16)
â”œâ”€â”€ Caching (precompute static features)
â””â”€â”€ Hardware upgrade (GPU inference, if budget allows)

Phase 3: Architecture Alternatives
â”œâ”€â”€ Two-stage model (fast filter â†’ detailed analysis)
â”œâ”€â”€ Async processing for non-blocking use cases
â”œâ”€â”€ Batch inference for historical analysis
â””â”€â”€ Hybrid: Rules for obvious cases, ML for uncertain
```

**Go/No-Go Criteria:**
- âœ… PROCEED: p99 < 100ms with acceptable F1-Score
- ğŸ›‘ STOP: Cannot achieve < 200ms even with simplest model â†’ Architecture redesign needed
- â†» PIVOT: Latency OK but accuracy suffers â†’ Accept accuracy trade-off OR async processing

**Owner:** ML Engineer + DevOps
**Status:** ğŸŸ¢ Benchmark Scheduled (Week 3)

---

### ğŸŸ¢ LOW RISKS (Accept and Monitor)

#### RISK 6: Interpretability Concerns

| Attribute | Details |
|-----------|---------|
| **Description** | Stakeholders or regulators require explanation of why transactions are flagged |
| **Probability** | 50% (depends on regulatory environment) |
| **Impact** | ğŸ”¥ ADOPTION RESISTANCE - Fraud team doesn't trust "black box" |

**Mitigation Plan:**
```
Level 1: Feature Importance (Default)
â”œâ”€â”€ Global importance: Which features matter most overall?
â”œâ”€â”€ Use: model.feature_importances_ for tree models
â”œâ”€â”€ Visualization: Bar chart of top 20 features
â””â”€â”€ Audience: Technical stakeholders, model documentation

Level 2: SHAP Values (If requested)
â”œâ”€â”€ Local importance: Why was THIS transaction flagged?
â”œâ”€â”€ Install: pip install shap
â”œâ”€â”€ Output: "V14 contributed +0.3, Amount contributed +0.2..."
â”œâ”€â”€ Visualization: Force plots, waterfall charts
â””â”€â”€ Audience: Fraud analysts, compliance, customer disputes

Level 3: Rule Extraction (If required by regulation)
â”œâ”€â”€ Convert complex model to approximate decision rules
â”œâ”€â”€ Tools: sklearn decision tree as surrogate
â”œâ”€â”€ Trade-off: Reduced accuracy for full transparency
â””â”€â”€ Audience: Regulators, legal team
```

**Owner:** ML Engineer
**Status:** ğŸŸ¢ SHAP Implementation Available

---

## 6. SUCCESS CRITERIA (Tiered)

### Tier 1: Minimum Viable Model (MVP)
**Gate: Week 4 | Must achieve ALL to proceed**

| Criterion | Threshold | Measurement | Status |
|-----------|-----------|-------------|--------|
| Precision | â‰¥ 50% | Test set evaluation | ğŸŸ¡ Pending |
| Recall | â‰¥ 75% | Test set evaluation | ğŸŸ¡ Pending |
| F1-Score | â‰¥ 65% | Test set evaluation | ğŸŸ¡ Pending |
| Inference Latency | < 100ms (p99) | Benchmark on prod hardware | ğŸŸ¡ Pending |
| No Data Leakage | Temporal validation within 5% of random | Audit | ğŸŸ¡ Pending |
| No Demographic Bias | Equal error rates (if data available) | Fairness audit | ğŸŸ¡ Pending |

**If NOT achieved:**
- Re-evaluate data quality and feature engineering
- Consider alternative algorithms
- Extend timeline by 2 weeks for remediation
- If still failing â†’ Escalate to stakeholders for scope discussion

---

### Tier 2: Production-Ready Model
**Gate: Week 6 | Should achieve for launch**

| Criterion | Threshold | Measurement | Status |
|-----------|-----------|-------------|--------|
| Precision | â‰¥ 60% | Test set evaluation | ğŸŸ¡ Pending |
| Recall | â‰¥ 85% | Test set evaluation | ğŸŸ¡ Pending |
| F1-Score | â‰¥ 70% | Test set evaluation | ğŸŸ¡ Pending |
| PR-AUC | â‰¥ 0.80 | Test set evaluation | ğŸŸ¡ Pending |
| ROI Projection | â‰¥ 3:1 | Business case validation | ğŸŸ¡ Pending |
| Stakeholder Approval | > 80% confidence | Sign-off meeting | ğŸŸ¡ Pending |
| Documentation | Complete | Checklist review | ğŸŸ¡ Pending |

**If NOT achieved:**
- Launch with MVP thresholds + enhanced monitoring
- Plan iteration cycle within 4 weeks of launch
- Document gaps and improvement roadmap

---

### Tier 3: Stretch Goals
**Timeline: Post-launch | Nice to have**

| Goal | Target | Business Value |
|------|--------|----------------|
| Precision | â‰¥ 70% | 20,000 fewer wasted reviews/year |
| Recall | â‰¥ 90% | Additional $200K fraud prevented |
| F1-Score | â‰¥ 80% | Best-in-class performance |
| Automated Retraining | CI/CD pipeline | Reduced maintenance burden |
| Real-time Dashboard | Grafana/similar | Operational visibility |
| A/B Testing Framework | Feature flags | Continuous improvement |

---

## 7. DECISION FRAMEWORK

### Go/No-Go Gates

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        GATE 1: After EDA (Week 2)                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                 â•‘
â•‘  âœ… PROCEED IF:                                                                â•‘
â•‘     â–¡ Data quality acceptable (< 10% missing values)                           â•‘
â•‘     â–¡ Clear separation visible between fraud/normal in feature distributions   â•‘
â•‘     â–¡ No critical data pipeline issues identified                              â•‘
â•‘     â–¡ Feature correlations with target exist (top features |r| > 0.1)         â•‘
â•‘                                                                                 â•‘
â•‘  ğŸ›‘ STOP IF:                                                                   â•‘
â•‘     â–¡ > 20% data unusable or corrupted                                         â•‘
â•‘     â–¡ No visible difference between fraud/normal distributions                 â•‘
â•‘     â–¡ Fundamental data collection issues (wrong time period, etc.)            â•‘
â•‘     â–¡ Data cannot be obtained in production (feature unavailable at decision) â•‘
â•‘                                                                                 â•‘
â•‘  â†» PIVOT IF:                                                                   â•‘
â•‘     â–¡ Some features useful, others not â†’ Feature selection focus              â•‘
â•‘     â–¡ Certain fraud types detectable, others not â†’ Segment approach           â•‘
â•‘     â–¡ External data needed â†’ Pause modeling, acquire data                     â•‘
â•‘                                                                                 â•‘
â•‘  DECISION OWNER: Project Lead + Data Engineer                                  â•‘
â•‘  DOCUMENTATION: EDA Report with go/no-go recommendation                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    GATE 2: After Baseline Models (Week 4)                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                 â•‘
â•‘  âœ… PROCEED IF:                                                                â•‘
â•‘     â–¡ At least one model achieves F1-Score â‰¥ 65%                              â•‘
â•‘     â–¡ Recall â‰¥ 75% (catching most fraud)                                      â•‘
â•‘     â–¡ No obvious data leakage detected (temporal val within 5% of random)     â•‘
â•‘     â–¡ Feature importance shows reasonable patterns                             â•‘
â•‘                                                                                 â•‘
â•‘  ğŸ›‘ STOP IF:                                                                   â•‘
â•‘     â–¡ F1-Score < 50% despite trying all sampling strategies                   â•‘
â•‘     â–¡ Model performance no better than random baseline                         â•‘
â•‘     â–¡ Severe data leakage discovered (inflated metrics)                       â•‘
â•‘     â–¡ Fundamental problem framing issue identified                            â•‘
â•‘                                                                                 â•‘
â•‘  â†» PIVOT IF:                                                                   â•‘
â•‘     â–¡ Good performance on random split, poor on temporal â†’ Drift issue        â•‘
â•‘     â–¡ One model type clearly superior â†’ Focus resources                       â•‘
â•‘     â–¡ Certain transaction types work, others don't â†’ Segment models           â•‘
â•‘                                                                                 â•‘
â•‘  DECISION OWNER: Project Lead + ML Engineer                                    â•‘
â•‘  DOCUMENTATION: Baseline Model Report with metrics comparison                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     GATE 3: After Optimization (Week 6)                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                 â•‘
â•‘  âœ… PROCEED IF:                                                                â•‘
â•‘     â–¡ F1-Score â‰¥ 70%, PR-AUC â‰¥ 0.80                                           â•‘
â•‘     â–¡ Inference latency < 100ms (p99)                                         â•‘
â•‘     â–¡ All audit checks passed (no leakage, acceptable bias)                   â•‘
â•‘     â–¡ Model behavior reasonable (SHAP explanations make sense)                â•‘
â•‘                                                                                 â•‘
â•‘  ğŸ›‘ STOP IF:                                                                   â•‘
â•‘     â–¡ Cannot meet latency AND performance simultaneously                      â•‘
â•‘     â–¡ Significant bias detected across demographics                           â•‘
â•‘     â–¡ Cost of deployment exceeds projected savings                            â•‘
â•‘     â–¡ Regulatory compliance cannot be achieved                                â•‘
â•‘                                                                                 â•‘
â•‘  â†» PIVOT IF:                                                                   â•‘
â•‘     â–¡ Performance good but slow â†’ Model compression, simpler model            â•‘
â•‘     â–¡ Fast but less accurate â†’ Hybrid rule + ML approach                      â•‘
â•‘     â–¡ Stakeholder trust issues â†’ Add human-in-the-loop for edge cases        â•‘
â•‘                                                                                 â•‘
â•‘  DECISION OWNER: Project Lead + Stakeholders                                   â•‘
â•‘  DOCUMENTATION: Optimization Report with production readiness assessment       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     GATE 4: Staging Deployment (Week 8)                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                 â•‘
â•‘  âœ… LAUNCH IF:                                                                 â•‘
â•‘     â–¡ Shadow mode testing shows â‰¥ 30% improvement vs current system           â•‘
â•‘     â–¡ Stakeholder approval obtained (â‰¥ 80% confidence)                        â•‘
â•‘     â–¡ Monitoring infrastructure operational                                    â•‘
â•‘     â–¡ Rollback plan documented and tested                                     â•‘
â•‘     â–¡ On-call rotation established                                            â•‘
â•‘                                                                                 â•‘
â•‘  ğŸ›‘ STOP IF:                                                                   â•‘
â•‘     â–¡ Shadow testing shows degradation vs current system                      â•‘
â•‘     â–¡ Production environment reveals critical issues                          â•‘
â•‘     â–¡ Regulatory approval not obtained                                        â•‘
â•‘     â–¡ Stakeholders withdraw support                                           â•‘
â•‘                                                                                 â•‘
â•‘  â†» PIVOT IF:                                                                   â•‘
â•‘     â–¡ Adoption concerns â†’ Phased rollout (10% â†’ 25% â†’ 50% â†’ 100%)            â•‘
â•‘     â–¡ Trust issues â†’ Human review for high-stakes decisions                   â•‘
â•‘     â–¡ Edge cases problematic â†’ Rules for known patterns, ML for rest         â•‘
â•‘                                                                                 â•‘
â•‘  DECISION OWNER: Project Lead + VP Engineering + CFO                           â•‘
â•‘  DOCUMENTATION: Launch Checklist + Runbook                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 8. PROJECT TIMELINE

### Gantt Chart Overview

```
Week    1    2    3    4    5    6    7    8    9    10
        â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
        â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚ Phase 1: EDA & Data Quality
        â”‚    â”‚    â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚    â”‚    â”‚    â”‚    â”‚    â”‚ Phase 2: Baseline Models
        â”‚    â”‚    â”‚    â”‚    â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚    â”‚    â”‚    â”‚ Phase 3: Optimization
        â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚â–“â–“â–“â–“â–“â–“â–“â–“â–“â”‚    â”‚ Phase 4: Staging & Testing
        â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚â–“â–“â–“â–“â”‚ Phase 5: Production Launch
        â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚    â”‚
        â”‚  G1â”‚    â”‚  G2â”‚    â”‚  G3â”‚    â”‚  G4â”‚    â”‚ GO â”‚ Gates
        â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

Legend: â–“ = Active work, G = Go/No-Go Gate
```

### Detailed Phase Breakdown

| Week | Phase | Key Activities | Deliverables | Gate |
|------|-------|----------------|--------------|------|
| 1 | EDA | Data loading, quality checks, missing value analysis | Data Quality Report | - |
| 2 | EDA | Feature analysis, correlation study, imbalance assessment | EDA Notebook, Visualizations | **G1** |
| 3 | Baseline | Train LR, RF, XGBoost with SMOTE/Undersampling/Weights | Baseline metrics | - |
| 4 | Baseline | Model comparison, leakage audit, temporal validation | Baseline Model Report | **G2** |
| 5 | Optimization | Hyperparameter tuning, threshold optimization | Tuned model | - |
| 6 | Optimization | Fairness audit, interpretability (SHAP), final evaluation | Optimization Report | **G3** |
| 7 | Staging | Deploy to staging, shadow mode testing, monitoring setup | Staging deployment | - |
| 8 | Staging | A/B testing, stakeholder demo, documentation finalization | Launch Checklist | **G4** |
| 9-10 | Launch | Production deployment, monitoring, initial support | Live system | - |

### Critical Path

```
Data Quality â”€â”€â”€â”€â”€â”€â–º Must pass before modeling begins
       â”‚
       â–¼
Baseline Model â”€â”€â”€â”€â–º Must achieve MVP metrics
       â”‚
       â–¼
Optimization â”€â”€â”€â”€â”€â”€â–º Must meet latency requirements
       â”‚
       â–¼
Staging Tests â”€â”€â”€â”€â”€â–º Must show improvement over current system
       â”‚
       â–¼
Production Launch
```

### Dependencies & Risks to Timeline

| Dependency | Risk | Mitigation | Impact if Delayed |
|------------|------|------------|-------------------|
| Data access | Medium | Early data pipeline validation | +1-2 weeks |
| Stakeholder availability | Medium | Schedule reviews in advance | +1 week per review |
| Infrastructure provisioning | Low | Use existing cloud resources | +1 week |
| Regulatory approval | High | Start compliance review Week 4 | +2-4 weeks |

---

## 9. TEAM & RESPONSIBILITIES

### RACI Matrix

| Activity | Project Lead | ML Engineer | Data Engineer | Fraud Team | Compliance |
|----------|:------------:|:-----------:|:-------------:|:----------:|:----------:|
| Project planning | **A** | C | C | I | I |
| Data quality audit | A | C | **R** | C | I |
| EDA & visualization | A | **R** | C | C | I |
| Model development | A | **R** | I | C | I |
| Hyperparameter tuning | I | **R** | I | I | I |
| Fairness & bias audit | A | **R** | I | C | **C** |
| Interpretability (SHAP) | I | **R** | I | C | C |
| Staging deployment | A | R | **R** | I | I |
| A/B testing | A | **R** | C | **C** | I |
| Monitoring setup | A | C | **R** | I | I |
| Documentation | A | **R** | C | C | C |
| Stakeholder communication | **R** | C | I | C | C |
| Go/No-Go decisions | **A** | C | C | C | C |

**Legend:** R = Responsible, A = Accountable, C = Consulted, I = Informed

### Team Allocation

| Role | Allocation | Weekly Hours | Key Responsibilities |
|------|------------|--------------|---------------------|
| Project Lead | 50% | 20 hrs | Strategy, stakeholder management, decisions |
| ML Engineer | 100% | 40 hrs | Model development, optimization, deployment |
| Data Engineer | 50% | 20 hrs | Data pipeline, quality, infrastructure |
| Fraud Team Lead | 25% | 10 hrs | Domain expertise, validation, feedback |
| Compliance Officer | 10% | 4 hrs | Regulatory review, approval |

### Escalation Path

```
Level 1: Technical Issues
ML Engineer â†’ Project Lead
Resolution: Within 24 hours

Level 2: Resource/Timeline Issues
Project Lead â†’ VP Engineering
Resolution: Within 48 hours

Level 3: Business/Strategic Issues
VP Engineering â†’ CFO/CRO
Resolution: Within 1 week
```

---

## 10. MONITORING & MAINTENANCE PLAN

### Production Monitoring Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRAUD DETECTION MODEL HEALTH                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  ğŸ“Š REAL-TIME METRICS (Last 24 hours)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Predictions â”‚   Latency   â”‚  Fraud Rate â”‚   Alerts    â”‚          â”‚
â”‚  â”‚   45,230    â”‚   23ms p50  â”‚    0.18%    â”‚     2       â”‚          â”‚
â”‚  â”‚   â–² 5%      â”‚   67ms p99  â”‚   â–¬ stable  â”‚   âš  review  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                       â”‚
â”‚  ğŸ“ˆ WEEKLY PERFORMANCE                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Precision  â”‚   Recall    â”‚  F1-Score   â”‚   PR-AUC    â”‚          â”‚
â”‚  â”‚    62.3%    â”‚   83.7%     â”‚   71.4%     â”‚   0.812     â”‚          â”‚
â”‚  â”‚  â–² +1.2%    â”‚  â–¼ -0.8%    â”‚  â–² +0.3%    â”‚  â–¬ stable   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                       â”‚
â”‚  ğŸ¯ VS BASELINE (Deployment: Jan 2026)                               â”‚
â”‚  Precision: +312% â”‚ Recall: +24% â”‚ F1: +186%                         â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Monitoring Schedule

| Frequency | Metrics | Owner | Action Threshold |
|-----------|---------|-------|------------------|
| **Real-time** | Prediction count, latency, error rate | Automated | Error rate > 1% â†’ Page on-call |
| **Hourly** | Fraud probability distribution | Automated | Mean shift > 2Ïƒ â†’ Alert |
| **Daily** | Prediction volume trends | Data Engineer | > 20% change â†’ Investigate |
| **Weekly** | Precision, Recall, F1, PR-AUC | ML Engineer | F1 drop > 3% â†’ Review |
| **Monthly** | Full performance audit, drift analysis | ML Engineer + Fraud Team | F1 drop > 5% â†’ Retrain |
| **Quarterly** | Model refresh, feature review | Full Team | Scheduled retrain |

### Alert Definitions

```python
# Alert Configuration
ALERTS = {
    "latency_high": {
        "condition": "p99_latency > 100ms for 5 minutes",
        "severity": "HIGH",
        "action": "Page on-call, consider traffic shedding"
    },
    "error_rate_high": {
        "condition": "error_rate > 1% for 10 minutes",
        "severity": "CRITICAL",
        "action": "Page on-call, activate fallback rules"
    },
    "prediction_drift": {
        "condition": "mean_fraud_prob change > 50% vs yesterday",
        "severity": "MEDIUM",
        "action": "Investigate data pipeline, alert ML Engineer"
    },
    "performance_degradation": {
        "condition": "weekly_f1 < 0.65",
        "severity": "HIGH",
        "action": "Trigger model review, consider retraining"
    }
}
```

### Maintenance Schedule

| Cadence | Activity | Owner | Duration |
|---------|----------|-------|----------|
| Weekly | Performance review meeting | ML Engineer + Fraud Team | 30 min |
| Monthly | Model drift analysis | ML Engineer | 4 hrs |
| Monthly | Retraining evaluation | ML Engineer | 8 hrs |
| Quarterly | Feature engineering review | Full Team | 1 day |
| Quarterly | Architecture review | ML Engineer + DevOps | 4 hrs |
| Annually | Full model rebuild | Full Team | 2-4 weeks |

### Retraining Protocol

```
AUTOMATIC RETRAINING TRIGGER:
â”œâ”€â”€ Condition: F1-Score drops > 5% from baseline for 2 consecutive weeks
â”œâ”€â”€ OR: Precision OR Recall drops > 10%
â”œâ”€â”€ OR: Quarterly scheduled retrain
â”‚
RETRAINING PROCESS:
â”œâ”€â”€ 1. Collect last 6 months of labeled transaction data
â”œâ”€â”€ 2. Run full training pipeline (same hyperparameters)
â”œâ”€â”€ 3. Evaluate on holdout set from most recent month
â”œâ”€â”€ 4. If new_model_f1 > current_model_f1:
â”‚       â””â”€â”€ Deploy to shadow mode (10% traffic) for 1 week
â”‚       â””â”€â”€ If shadow performance good â†’ Gradual rollout
â”œâ”€â”€ 5. If new_model_f1 â‰¤ current_model_f1:
â”‚       â””â”€â”€ Investigate (data quality? fraud pattern change?)
â”‚       â””â”€â”€ Consider hyperparameter re-tuning
â”‚       â””â”€â”€ Escalate if no improvement after 2 attempts
â”‚
ROLLBACK PLAN:
â”œâ”€â”€ Keep previous model version deployed in parallel
â”œâ”€â”€ Feature flag to switch traffic instantly
â”œâ”€â”€ Rollback decision: Production F1 < Shadow F1 by > 5%
â””â”€â”€ Rollback execution: < 5 minutes via feature flag
```

---

## 11. APPENDIX

### A. Glossary of Terms

| Term | Definition | Business Context |
|------|------------|------------------|
| **Precision** | TP / (TP + FP) | "Of transactions we flag, what % are actually fraud?" |
| **Recall** | TP / (TP + FN) | "Of all actual frauds, what % do we catch?" |
| **F1-Score** | 2 Ã— (P Ã— R) / (P + R) | Harmonic mean, balances precision and recall |
| **PR-AUC** | Area under Precision-Recall curve | Overall model quality, robust to imbalance |
| **ROC-AUC** | Area under ROC curve | Less useful for imbalanced data (can be misleading) |
| **SMOTE** | Synthetic Minority Over-sampling | Creates artificial fraud examples for training |
| **Class Weights** | Penalty multiplier for classes | Makes model care more about minority class |
| **Data Leakage** | Future info in training data | Causes inflated metrics, production failure |
| **Model Drift** | Performance decay over time | Fraudsters adapt, patterns change |
| **SHAP Values** | Feature contribution scores | Explains why a specific prediction was made |

### B. Technical Specifications

**Model Configuration (XGBoost - Recommended):**
```python
{
    "n_estimators": 100,
    "max_depth": 6,
    "learning_rate": 0.1,
    "scale_pos_weight": "auto",  # Calculated from class ratio
    "eval_metric": "logloss",
    "random_state": 42
}
```

**Prediction Threshold Strategy:**
```python
# Threshold tuning based on business cost
# Cost of False Negative (missed fraud): ~$500
# Cost of False Positive (wasted review): ~$10
# Cost ratio: 50:1

# Optimal threshold typically around 0.3-0.4 for fraud detection
# (lower than default 0.5 to catch more fraud at expense of more reviews)

thresholds = {
    "auto_block": 0.80,      # High confidence â†’ block immediately
    "manual_review": 0.40,   # Medium confidence â†’ human review
    "monitor": 0.20,         # Low confidence â†’ log for analysis
    "approve": 0.00          # Below threshold â†’ approve transaction
}
```

### C. Dataset Specifications

| Attribute | Value |
|-----------|-------|
| Source | Kaggle Credit Card Fraud Detection Dataset |
| Total Records | 284,807 transactions |
| Time Period | September 2013 (2 days) |
| Features | 30 (V1-V28 PCA components + Time + Amount) |
| Target | Class (0 = Normal, 1 = Fraud) |
| Class Distribution | 99.83% Normal, 0.17% Fraud |
| Missing Values | 0 |
| File Size | ~144 MB |

### D. References

1. **Dataset**: https://www.kaggle.com/mlg-ulb/creditcardfraud
2. **SMOTE Paper**: Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique" (2002)
3. **XGBoost Paper**: Chen & Guestrin, "XGBoost: A Scalable Tree Boosting System" (2016)
4. **SHAP Paper**: Lundberg & Lee, "A Unified Approach to Interpreting Model Predictions" (2017)
5. **Imbalanced Learning**: He & Garcia, "Learning from Imbalanced Data" (2009)

### E. Stakeholder Sign-Off

| Stakeholder | Role | Approval Status | Date |
|-------------|------|-----------------|------|
| [Name] | CFO | â¬œ Pending | - |
| [Name] | VP Engineering | â¬œ Pending | - |
| [Name] | Fraud Operations Manager | â¬œ Pending | - |
| [Name] | Chief Risk Officer | â¬œ Pending | - |
| [Name] | Chief Compliance Officer | â¬œ Pending | - |

---

**Document Control:**
- Version: 1.0
- Created: January 2026
- Last Updated: January 2026
- Next Review: After Gate 1 (Week 2)
- Owner: Project Lead
